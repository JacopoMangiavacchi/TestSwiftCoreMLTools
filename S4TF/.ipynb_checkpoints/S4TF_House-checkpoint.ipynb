{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO4FT9bUohHx"
   },
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "RgQazsBcohH5",
    "outputId": "c5f63b76-3009-4725-ef9d-caccf24671c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 14 101 14\r\n",
      "405 13 405 1 101 13 101 1\r\n",
      "5265 405 1313 101\r\n"
     ]
    }
   ],
   "source": [
    "let trainCSV = try String(contentsOfFile:\"../data/train.csv\", encoding: String.Encoding.utf8)\n",
    "let testCSV = try String(contentsOfFile:\"../data/test.csv\", encoding: String.Encoding.utf8)\n",
    "\n",
    "let trainRecords: [[Float]] = trainCSV.split(separator: \"\\n\").map{ String($0).split(separator: \",\").compactMap{ Float(String($0)) } }\n",
    "let testRecords: [[Float]] = testCSV.split(separator: \"\\n\").map{ String($0).split(separator: \",\").compactMap{ Float(String($0)) } }\n",
    "\n",
    "let numTrainRecords = trainRecords.count\n",
    "let numTrainColumns = trainRecords[0].count\n",
    "let numTestRecords = testRecords.count\n",
    "let numTestColumns = testRecords[0].count\n",
    "\n",
    "print(numTrainRecords, numTrainColumns, numTestRecords, numTestColumns)\n",
    "\n",
    "let xTrain = trainRecords.map{ Array($0[0..<numTrainColumns-1]) }\n",
    "let yTrain = trainRecords.map{ [$0[numTrainColumns-1]] }\n",
    "let xTest = testRecords.map{ Array($0[0..<numTestColumns-1]) }\n",
    "let yTest = testRecords.map{ [$0[numTestColumns-1]] }\n",
    "\n",
    "print(xTrain.count, xTrain[0].count, yTrain.count, yTrain[0].count,\n",
    "      xTest.count, xTest[0].count, yTest.count, yTest[0].count)\n",
    "\n",
    "let xAllTrain = Array(xTrain.joined())\n",
    "let yAllTrain = Array(yTrain.joined())\n",
    "let xAllTest = Array(xTest.joined())\n",
    "let yAllTest = Array(yTest.joined())\n",
    "\n",
    "print(xAllTrain.count, yAllTrain.count, xAllTest.count, yAllTest.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "qlhl7tOaohH_",
    "outputId": "1eef7f0c-fa9d-4f83-a1a0-8ea555fbe3b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[405, 13] [405, 1] [101, 13] [101, 1]\r\n"
     ]
    }
   ],
   "source": [
    "let XTrain = Tensor<Float>(xAllTrain).reshaped(to: TensorShape([numTrainRecords, numTrainColumns-1]))\n",
    "let YTrain = Tensor<Float>(yAllTrain)\n",
    "let XTest = Tensor<Float>(xAllTest).reshaped(to: TensorShape([numTestRecords, numTestColumns-1]))\n",
    "let YTest = Tensor<Float>(yAllTest)\n",
    "\n",
    "print(XTrain.shape, YTrain.shape, XTest.shape, YTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDJZCCgqohIC"
   },
   "outputs": [],
   "source": [
    "struct RegressionModel: Layer {\n",
    "    var layer1 = Dense<Float>(inputSize: 13, outputSize: 64, activation: relu)\n",
    "    var layer2 = Dense<Float>(inputSize: 64, outputSize: 32, activation: relu)\n",
    "    var layer3 = Dense<Float>(inputSize: 32, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
    "        return input.sequenced(through: layer1, layer2, layer3)\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = RegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK0Vj7bSohIF"
   },
   "outputs": [],
   "source": [
    "let optimizer = RMSProp(for: model, learningRate: 0.001)\n",
    "Context.local.learningPhase = .training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8C7yHJohIH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 566.6206\n",
      "Loss: 553.331\n",
      "Loss: 543.4708\n",
      "Loss: 534.99347\n",
      "Loss: 527.2901\n",
      "Loss: 520.0868\n",
      "Loss: 513.1813\n",
      "Loss: 506.51056\n",
      "Loss: 500.0162\n",
      "Loss: 493.61093\n",
      "Loss: 487.2897\n",
      "Loss: 481.02002\n",
      "Loss: 474.81122\n",
      "Loss: 468.6606\n",
      "Loss: 462.5218\n",
      "Loss: 456.4003\n",
      "Loss: 450.289\n",
      "Loss: 444.1698\n",
      "Loss: 438.05228\n",
      "Loss: 431.9293\n",
      "Loss: 425.80957\n",
      "Loss: 419.682\n",
      "Loss: 413.54623\n",
      "Loss: 407.39627\n",
      "Loss: 401.22852\n",
      "Loss: 395.0472\n",
      "Loss: 388.85355\n",
      "Loss: 382.6528\n",
      "Loss: 376.43964\n",
      "Loss: 370.2015\n",
      "Loss: 363.93536\n",
      "Loss: 357.65054\n",
      "Loss: 351.35977\n",
      "Loss: 345.06503\n",
      "Loss: 338.76944\n",
      "Loss: 332.47913\n",
      "Loss: 326.19696\n",
      "Loss: 319.92685\n",
      "Loss: 313.66766\n",
      "Loss: 307.42767\n",
      "Loss: 301.20773\n",
      "Loss: 295.00345\n",
      "Loss: 288.8232\n",
      "Loss: 282.67578\n",
      "Loss: 276.56247\n",
      "Loss: 270.49133\n",
      "Loss: 264.46695\n",
      "Loss: 258.4889\n",
      "Loss: 252.55869\n",
      "Loss: 246.69626\n",
      "Loss: 240.90317\n",
      "Loss: 235.18173\n",
      "Loss: 229.5378\n",
      "Loss: 223.97552\n",
      "Loss: 218.4955\n",
      "Loss: 213.10083\n",
      "Loss: 207.79228\n",
      "Loss: 202.57164\n",
      "Loss: 197.4394\n",
      "Loss: 192.39995\n",
      "Loss: 187.45427\n",
      "Loss: 182.60349\n",
      "Loss: 177.84882\n",
      "Loss: 173.18979\n",
      "Loss: 168.62502\n",
      "Loss: 164.15163\n",
      "Loss: 159.77563\n",
      "Loss: 155.49924\n",
      "Loss: 151.31982\n",
      "Loss: 147.2374\n",
      "Loss: 143.25316\n",
      "Loss: 139.36964\n",
      "Loss: 135.58485\n",
      "Loss: 131.89426\n",
      "Loss: 128.30217\n",
      "Loss: 124.806305\n",
      "Loss: 121.40951\n",
      "Loss: 118.10997\n",
      "Loss: 114.906425\n",
      "Loss: 111.80028\n",
      "Loss: 108.787346\n",
      "Loss: 105.867165\n",
      "Loss: 103.03916\n",
      "Loss: 100.30276\n",
      "Loss: 97.65703\n",
      "Loss: 95.10039\n",
      "Loss: 92.63072\n",
      "Loss: 90.243675\n",
      "Loss: 87.93881\n",
      "Loss: 85.7138\n",
      "Loss: 83.56667\n",
      "Loss: 81.49278\n",
      "Loss: 79.49145\n",
      "Loss: 77.55645\n",
      "Loss: 75.68306\n",
      "Loss: 73.86754\n",
      "Loss: 72.10642\n",
      "Loss: 70.397766\n",
      "Loss: 68.7386\n",
      "Loss: 67.12566\n",
      "Loss: 65.56024\n",
      "Loss: 64.03894\n",
      "Loss: 62.56025\n",
      "Loss: 61.120747\n",
      "Loss: 59.719486\n",
      "Loss: 58.355347\n",
      "Loss: 57.025005\n",
      "Loss: 55.727734\n",
      "Loss: 54.461327\n",
      "Loss: 53.22682\n",
      "Loss: 52.023796\n",
      "Loss: 50.850067\n",
      "Loss: 49.705925\n",
      "Loss: 48.59236\n",
      "Loss: 47.50774\n",
      "Loss: 46.45333\n",
      "Loss: 45.42975\n",
      "Loss: 44.43662\n",
      "Loss: 43.472725\n",
      "Loss: 42.535355\n",
      "Loss: 41.627487\n",
      "Loss: 40.748104\n",
      "Loss: 39.89764\n",
      "Loss: 39.075306\n",
      "Loss: 38.2815\n",
      "Loss: 37.514572\n",
      "Loss: 36.77508\n",
      "Loss: 36.063618\n",
      "Loss: 35.378853\n",
      "Loss: 34.721996\n",
      "Loss: 34.09131\n",
      "Loss: 33.48474\n",
      "Loss: 32.90087\n",
      "Loss: 32.34064\n",
      "Loss: 31.803879\n",
      "Loss: 31.290565\n",
      "Loss: 30.798983\n",
      "Loss: 30.325834\n",
      "Loss: 29.871862\n",
      "Loss: 29.436754\n",
      "Loss: 29.020826\n",
      "Loss: 28.622147\n",
      "Loss: 28.239895\n",
      "Loss: 27.87346\n",
      "Loss: 27.522709\n",
      "Loss: 27.18458\n",
      "Loss: 26.856419\n",
      "Loss: 26.540447\n",
      "Loss: 26.235588\n",
      "Loss: 25.94045\n",
      "Loss: 25.65267\n",
      "Loss: 25.370174\n",
      "Loss: 25.09211\n",
      "Loss: 24.818089\n",
      "Loss: 24.551252\n",
      "Loss: 24.290014\n",
      "Loss: 24.033758\n",
      "Loss: 23.783247\n",
      "Loss: 23.535217\n",
      "Loss: 23.29003\n",
      "Loss: 23.04877\n",
      "Loss: 22.808624\n",
      "Loss: 22.570925\n",
      "Loss: 22.337221\n",
      "Loss: 22.105398\n",
      "Loss: 21.875729\n",
      "Loss: 21.648499\n",
      "Loss: 21.421875\n",
      "Loss: 21.197874\n",
      "Loss: 20.974201\n",
      "Loss: 20.750795\n",
      "Loss: 20.530115\n",
      "Loss: 20.311735\n",
      "Loss: 20.095642\n",
      "Loss: 19.883133\n",
      "Loss: 19.671312\n",
      "Loss: 19.462425\n",
      "Loss: 19.255108\n",
      "Loss: 19.050482\n",
      "Loss: 18.847952\n",
      "Loss: 18.65083\n",
      "Loss: 18.466944\n",
      "Loss: 18.305882\n",
      "Loss: 18.15089\n",
      "Loss: 17.96298\n",
      "Loss: 17.752089\n",
      "Loss: 17.553904\n",
      "Loss: 17.365128\n",
      "Loss: 17.188759\n",
      "Loss: 17.01816\n",
      "Loss: 16.849604\n",
      "Loss: 16.685165\n",
      "Loss: 16.523647\n",
      "Loss: 16.365215\n",
      "Loss: 16.21239\n",
      "Loss: 16.063251\n",
      "Loss: 15.919364\n",
      "Loss: 15.781652\n",
      "Loss: 15.649788\n",
      "Loss: 15.515542\n",
      "Loss: 15.378017\n",
      "Loss: 15.234585\n",
      "Loss: 15.09614\n",
      "Loss: 14.958794\n",
      "Loss: 14.82828\n",
      "Loss: 14.70439\n",
      "Loss: 14.584844\n",
      "Loss: 14.466172\n",
      "Loss: 14.353758\n",
      "Loss: 14.240911\n",
      "Loss: 14.132898\n",
      "Loss: 14.029179\n",
      "Loss: 13.929347\n",
      "Loss: 13.832064\n",
      "Loss: 13.738235\n",
      "Loss: 13.641116\n",
      "Loss: 13.548581\n",
      "Loss: 13.453098\n",
      "Loss: 13.363358\n",
      "Loss: 13.271315\n",
      "Loss: 13.1852\n",
      "Loss: 13.100849\n",
      "Loss: 13.017478\n",
      "Loss: 12.938011\n",
      "Loss: 12.861471\n",
      "Loss: 12.788821\n",
      "Loss: 12.717586\n",
      "Loss: 12.6458\n",
      "Loss: 12.579754\n",
      "Loss: 12.506444\n",
      "Loss: 12.438998\n",
      "Loss: 12.36976\n",
      "Loss: 12.305919\n",
      "Loss: 12.24027\n",
      "Loss: 12.179344\n",
      "Loss: 12.116096\n",
      "Loss: 12.056788\n",
      "Loss: 11.998466\n",
      "Loss: 11.941748\n",
      "Loss: 11.887271\n",
      "Loss: 11.834894\n",
      "Loss: 11.779471\n",
      "Loss: 11.730157\n",
      "Loss: 11.677648\n",
      "Loss: 11.628048\n",
      "Loss: 11.577961\n",
      "Loss: 11.53288\n",
      "Loss: 11.491753\n",
      "Loss: 11.452826\n",
      "Loss: 11.405909\n",
      "Loss: 11.352378\n",
      "Loss: 11.297264\n",
      "Loss: 11.245697\n",
      "Loss: 11.197741\n",
      "Loss: 11.15608\n",
      "Loss: 11.116147\n",
      "Loss: 11.085971\n",
      "Loss: 11.057148\n",
      "Loss: 11.037741\n",
      "Loss: 10.998285\n",
      "Loss: 10.952746\n",
      "Loss: 10.892055\n",
      "Loss: 10.8362465\n",
      "Loss: 10.782244\n",
      "Loss: 10.735935\n",
      "Loss: 10.6899185\n",
      "Loss: 10.648701\n",
      "Loss: 10.606915\n",
      "Loss: 10.570072\n",
      "Loss: 10.531904\n",
      "Loss: 10.500511\n",
      "Loss: 10.469698\n",
      "Loss: 10.440468\n",
      "Loss: 10.414869\n",
      "Loss: 10.39137\n",
      "Loss: 10.362142\n",
      "Loss: 10.3305025\n",
      "Loss: 10.292046\n",
      "Loss: 10.251532\n",
      "Loss: 10.208579\n",
      "Loss: 10.162032\n",
      "Loss: 10.115454\n",
      "Loss: 10.077325\n",
      "Loss: 10.040994\n",
      "Loss: 10.0055485\n",
      "Loss: 9.974298\n",
      "Loss: 9.945124\n",
      "Loss: 9.921145\n",
      "Loss: 9.904069\n",
      "Loss: 9.890487\n",
      "Loss: 9.882202\n",
      "Loss: 9.862975\n",
      "Loss: 9.832534\n",
      "Loss: 9.789203\n",
      "Loss: 9.747206\n",
      "Loss: 9.710088\n",
      "Loss: 9.681347\n",
      "Loss: 9.660821\n",
      "Loss: 9.634965\n",
      "Loss: 9.601944\n",
      "Loss: 9.564824\n",
      "Loss: 9.525694\n",
      "Loss: 9.49451\n",
      "Loss: 9.465668\n",
      "Loss: 9.446341\n",
      "Loss: 9.43006\n",
      "Loss: 9.424531\n",
      "Loss: 9.428333\n",
      "Loss: 9.435653\n",
      "Loss: 9.413251\n",
      "Loss: 9.373376\n",
      "Loss: 9.327138\n",
      "Loss: 9.283869\n",
      "Loss: 9.249405\n",
      "Loss: 9.215381\n",
      "Loss: 9.184866\n",
      "Loss: 9.157428\n",
      "Loss: 9.132821\n",
      "Loss: 9.108534\n",
      "Loss: 9.087454\n",
      "Loss: 9.072297\n",
      "Loss: 9.062642\n",
      "Loss: 9.061839\n",
      "Loss: 9.066185\n",
      "Loss: 9.068612\n",
      "Loss: 9.039435\n",
      "Loss: 9.001991\n",
      "Loss: 8.9528675\n",
      "Loss: 8.916982\n",
      "Loss: 8.888064\n",
      "Loss: 8.869243\n",
      "Loss: 8.849332\n",
      "Loss: 8.830499\n",
      "Loss: 8.799462\n",
      "Loss: 8.76982\n",
      "Loss: 8.737939\n",
      "Loss: 8.713406\n",
      "Loss: 8.688558\n",
      "Loss: 8.670404\n",
      "Loss: 8.652929\n",
      "Loss: 8.65223\n",
      "Loss: 8.668117\n",
      "Loss: 8.700013\n",
      "Loss: 8.692357\n",
      "Loss: 8.649789\n",
      "Loss: 8.589776\n",
      "Loss: 8.549206\n",
      "Loss: 8.514505\n",
      "Loss: 8.4808445\n",
      "Loss: 8.457491\n",
      "Loss: 8.432108\n",
      "Loss: 8.40869\n",
      "Loss: 8.386143\n",
      "Loss: 8.370085\n",
      "Loss: 8.3545885\n",
      "Loss: 8.345171\n",
      "Loss: 8.334209\n",
      "Loss: 8.3324375\n",
      "Loss: 8.343765\n",
      "Loss: 8.356752\n",
      "Loss: 8.34688\n",
      "Loss: 8.298227\n",
      "Loss: 8.250772\n",
      "Loss: 8.209149\n",
      "Loss: 8.180614\n",
      "Loss: 8.160719\n",
      "Loss: 8.1534\n",
      "Loss: 8.141945\n",
      "Loss: 8.1318245\n",
      "Loss: 8.106751\n",
      "Loss: 8.082451\n",
      "Loss: 8.055276\n",
      "Loss: 8.03209\n",
      "Loss: 8.009536\n",
      "Loss: 7.997376\n",
      "Loss: 7.9915376\n",
      "Loss: 8.003276\n",
      "Loss: 8.033122\n",
      "Loss: 8.06312\n",
      "Loss: 8.031846\n",
      "Loss: 7.9838066\n",
      "Loss: 7.9361124\n",
      "Loss: 7.8994865\n",
      "Loss: 7.8680935\n",
      "Loss: 7.8422284\n",
      "Loss: 7.8241796\n",
      "Loss: 7.8043528\n",
      "Loss: 7.788777\n",
      "Loss: 7.7733665\n",
      "Loss: 7.762935\n",
      "Loss: 7.751921\n",
      "Loss: 7.7452464\n",
      "Loss: 7.747826\n",
      "Loss: 7.756889\n",
      "Loss: 7.7727637\n",
      "Loss: 7.7783346\n",
      "Loss: 7.7629848\n",
      "Loss: 7.7152267\n",
      "Loss: 7.673515\n",
      "Loss: 7.637667\n",
      "Loss: 7.613335\n",
      "Loss: 7.598191\n",
      "Loss: 7.5905514\n",
      "Loss: 7.5852237\n",
      "Loss: 7.57337\n",
      "Loss: 7.5526485\n",
      "Loss: 7.5377035\n",
      "Loss: 7.5143695\n",
      "Loss: 7.49347\n",
      "Loss: 7.474925\n",
      "Loss: 7.460494\n",
      "Loss: 7.449038\n",
      "Loss: 7.446346\n",
      "Loss: 7.4558005\n",
      "Loss: 7.497297\n",
      "Loss: 7.556261\n",
      "Loss: 7.5548997\n",
      "Loss: 7.4816046\n",
      "Loss: 7.4265046\n",
      "Loss: 7.3864617\n",
      "Loss: 7.357468\n",
      "Loss: 7.3371897\n",
      "Loss: 7.3184123\n",
      "Loss: 7.304692\n",
      "Loss: 7.2896805\n",
      "Loss: 7.2814527\n",
      "Loss: 7.269802\n",
      "Loss: 7.2591386\n",
      "Loss: 7.256742\n",
      "Loss: 7.2627654\n",
      "Loss: 7.272241\n",
      "Loss: 7.2869105\n",
      "Loss: 7.3025184\n",
      "Loss: 7.292056\n",
      "Loss: 7.2623305\n",
      "Loss: 7.217588\n",
      "Loss: 7.1840935\n",
      "Loss: 7.159045\n",
      "Loss: 7.1484003\n",
      "Loss: 7.1417217\n",
      "Loss: 7.137677\n",
      "Loss: 7.127446\n",
      "Loss: 7.112132\n",
      "Loss: 7.0951257\n",
      "Loss: 7.0794053\n",
      "Loss: 7.061337\n",
      "Loss: 7.04525\n",
      "Loss: 7.0316563\n",
      "Loss: 7.0213027\n",
      "Loss: 7.011262\n",
      "Loss: 7.0120792\n",
      "Loss: 7.0255384\n",
      "Loss: 7.0737004\n",
      "Loss: 7.1382256\n",
      "Loss: 7.1403003\n",
      "Loss: 7.068238\n",
      "Loss: 7.010208\n",
      "Loss: 6.9676256\n",
      "Loss: 6.9388967\n",
      "Loss: 6.9230704\n",
      "Loss: 6.9041986\n",
      "Loss: 6.889682\n",
      "Loss: 6.8789434\n",
      "Loss: 6.8699455\n",
      "Loss: 6.863268\n",
      "Loss: 6.865164\n",
      "Loss: 6.864021\n",
      "Loss: 6.8673334\n",
      "Loss: 6.8771234\n",
      "Loss: 6.8971577\n",
      "Loss: 6.9145417\n",
      "Loss: 6.903788\n",
      "Loss: 6.87793\n",
      "Loss: 6.8347034\n",
      "Loss: 6.8016224\n",
      "Loss: 6.7765336\n",
      "Loss: 6.7626705\n",
      "Loss: 6.7569895\n",
      "Loss: 6.7558455\n",
      "Loss: 6.7528663\n",
      "Loss: 6.751878\n",
      "Loss: 6.73544\n",
      "Loss: 6.7133536\n",
      "Loss: 6.6922574\n",
      "Loss: 6.677962\n",
      "Loss: 6.661974\n",
      "Loss: 6.65168\n",
      "Loss: 6.6397443\n",
      "Loss: 6.632918\n",
      "Loss: 6.629376\n",
      "Loss: 6.631712\n",
      "Loss: 6.636946\n",
      "Loss: 6.6753836\n",
      "Loss: 6.768612\n",
      "Loss: 6.831662\n",
      "Loss: 6.746506\n",
      "Loss: 6.6251264\n",
      "Loss: 6.566146\n",
      "Loss: 6.535606\n",
      "Loss: 6.5185184\n",
      "Loss: 6.5037646\n",
      "Loss: 6.493634\n",
      "Loss: 6.485947\n",
      "Loss: 6.4841485\n",
      "Loss: 6.4915276\n",
      "Loss: 6.5078325\n",
      "Loss: 6.5282316\n",
      "Loss: 6.5378966\n",
      "Loss: 6.512051\n",
      "Loss: 6.4817867\n",
      "Loss: 6.44973\n",
      "Loss: 6.431572\n",
      "Loss: 6.425809\n",
      "Loss: 6.436179\n",
      "Loss: 6.4641824\n",
      "Loss: 6.507774\n",
      "Loss: 6.510609\n",
      "Loss: 6.4743805\n",
      "Loss: 6.4198093\n",
      "Loss: 6.3757005\n",
      "Loss: 6.345765\n",
      "Loss: 6.3268943\n",
      "Loss: 6.31576\n",
      "Loss: 6.305016\n",
      "Loss: 6.300435\n",
      "Loss: 6.298266\n",
      "Loss: 6.3037124\n",
      "Loss: 6.3119802\n",
      "Loss: 6.329444\n",
      "Loss: 6.3449693\n",
      "Loss: 6.3530073\n",
      "Loss: 6.3539567\n",
      "Loss: 6.3333015\n",
      "Loss: 6.3049293\n",
      "Loss: 6.269942\n",
      "Loss: 6.25061\n",
      "Loss: 6.2355227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.2359343\n",
      "Loss: 6.230207\n",
      "Loss: 6.221217\n",
      "Loss: 6.2039847\n",
      "Loss: 6.1909933\n",
      "Loss: 6.1701064\n",
      "Loss: 6.158089\n",
      "Loss: 6.1459184\n",
      "Loss: 6.139557\n",
      "Loss: 6.1379027\n",
      "Loss: 6.1492796\n",
      "Loss: 6.1817956\n",
      "Loss: 6.247212\n",
      "Loss: 6.289013\n",
      "Loss: 6.2442827\n",
      "Loss: 6.1772695\n",
      "Loss: 6.120755\n",
      "Loss: 6.0853825\n",
      "Loss: 6.061435\n",
      "Loss: 6.050526\n",
      "Loss: 6.0423017\n",
      "Loss: 6.038246\n",
      "Loss: 6.0364156\n",
      "Loss: 6.0456495\n",
      "Loss: 6.0624704\n",
      "Loss: 6.088698\n",
      "Loss: 6.100321\n",
      "Loss: 6.088906\n",
      "Loss: 6.0458326\n",
      "Loss: 6.0121975\n",
      "Loss: 5.9852557\n",
      "Loss: 5.9736123\n",
      "Loss: 5.9644732\n",
      "Loss: 5.9650254\n",
      "Loss: 5.9877024\n",
      "Loss: 6.048351\n",
      "Loss: 6.115866\n",
      "Loss: 6.1170206\n",
      "Loss: 6.05297\n",
      "Loss: 5.9792404\n",
      "Loss: 5.936389\n",
      "Loss: 5.911039\n",
      "Loss: 5.893418\n",
      "Loss: 5.882082\n",
      "Loss: 5.87834\n",
      "Loss: 5.8749185\n",
      "Loss: 5.8763213\n",
      "Loss: 5.879794\n",
      "Loss: 5.8925433\n",
      "Loss: 5.906451\n",
      "Loss: 5.9207315\n",
      "Loss: 5.918136\n",
      "Loss: 5.9169717\n",
      "Loss: 5.906542\n",
      "Loss: 5.889885\n",
      "Loss: 5.8639903\n",
      "Loss: 5.8472624\n",
      "Loss: 5.8326297\n",
      "Loss: 5.831289\n",
      "Loss: 5.8370547\n",
      "Loss: 5.8555617\n",
      "Loss: 5.8685317\n",
      "Loss: 5.8656263\n",
      "Loss: 5.8469176\n",
      "Loss: 5.817493\n",
      "Loss: 5.7931414\n",
      "Loss: 5.7725697\n",
      "Loss: 5.756188\n",
      "Loss: 5.7435536\n",
      "Loss: 5.7415032\n",
      "Loss: 5.740131\n",
      "Loss: 5.742975\n",
      "Loss: 5.7506084\n",
      "Loss: 5.762195\n",
      "Loss: 5.779918\n",
      "Loss: 5.7872257\n",
      "Loss: 5.777612\n",
      "Loss: 5.7489862\n",
      "Loss: 5.7223415\n",
      "Loss: 5.6909723\n",
      "Loss: 5.669855\n",
      "Loss: 5.660675\n",
      "Loss: 5.658329\n",
      "Loss: 5.6645956\n",
      "Loss: 5.6839\n",
      "Loss: 5.716548\n",
      "Loss: 5.7468057\n",
      "Loss: 5.7448416\n",
      "Loss: 5.7180943\n",
      "Loss: 5.677361\n",
      "Loss: 5.6524353\n",
      "Loss: 5.6334815\n",
      "Loss: 5.6296167\n",
      "Loss: 5.632787\n",
      "Loss: 5.635676\n",
      "Loss: 5.633221\n",
      "Loss: 5.622795\n",
      "Loss: 5.6100235\n",
      "Loss: 5.591958\n",
      "Loss: 5.5796814\n",
      "Loss: 5.567591\n",
      "Loss: 5.560241\n",
      "Loss: 5.5567956\n",
      "Loss: 5.5585775\n",
      "Loss: 5.565899\n",
      "Loss: 5.581092\n",
      "Loss: 5.5999994\n",
      "Loss: 5.6030116\n",
      "Loss: 5.5933056\n",
      "Loss: 5.5575233\n",
      "Loss: 5.5318437\n",
      "Loss: 5.5115047\n",
      "Loss: 5.5077267\n",
      "Loss: 5.5191593\n",
      "Loss: 5.5352745\n",
      "Loss: 5.536634\n",
      "Loss: 5.5163574\n",
      "Loss: 5.4871407\n",
      "Loss: 5.46431\n",
      "Loss: 5.445905\n",
      "Loss: 5.4424815\n",
      "Loss: 5.4460444\n",
      "Loss: 5.472488\n",
      "Loss: 5.518507\n",
      "Loss: 5.5540347\n",
      "Loss: 5.5507283\n",
      "Loss: 5.494008\n",
      "Loss: 5.444843\n",
      "Loss: 5.410416\n",
      "Loss: 5.389416\n",
      "Loss: 5.373175\n",
      "Loss: 5.367076\n",
      "Loss: 5.3617\n",
      "Loss: 5.3631916\n",
      "Loss: 5.3676286\n",
      "Loss: 5.3773727\n",
      "Loss: 5.3926597\n",
      "Loss: 5.4111943\n",
      "Loss: 5.4346952\n",
      "Loss: 5.4512672\n",
      "Loss: 5.4471507\n",
      "Loss: 5.40727\n",
      "Loss: 5.364134\n",
      "Loss: 5.3266277\n",
      "Loss: 5.309872\n",
      "Loss: 5.3007364\n",
      "Loss: 5.3036613\n",
      "Loss: 5.314788\n",
      "Loss: 5.3329453\n",
      "Loss: 5.348388\n",
      "Loss: 5.351882\n",
      "Loss: 5.3320317\n",
      "Loss: 5.3083925\n",
      "Loss: 5.2851024\n",
      "Loss: 5.272165\n",
      "Loss: 5.270602\n",
      "Loss: 5.2806377\n",
      "Loss: 5.29748\n",
      "Loss: 5.313034\n",
      "Loss: 5.308999\n",
      "Loss: 5.2809925\n",
      "Loss: 5.2504725\n",
      "Loss: 5.223289\n",
      "Loss: 5.202487\n",
      "Loss: 5.1924415\n",
      "Loss: 5.1859612\n",
      "Loss: 5.1880317\n",
      "Loss: 5.1937838\n",
      "Loss: 5.213258\n",
      "Loss: 5.245636\n",
      "Loss: 5.2814093\n",
      "Loss: 5.274759\n",
      "Loss: 5.2459655\n",
      "Loss: 5.195121\n",
      "Loss: 5.1650567\n",
      "Loss: 5.148695\n",
      "Loss: 5.149647\n",
      "Loss: 5.172858\n",
      "Loss: 5.20049\n",
      "Loss: 5.192244\n",
      "Loss: 5.1677055\n",
      "Loss: 5.1322794\n",
      "Loss: 5.108869\n",
      "Loss: 5.093725\n",
      "Loss: 5.0885205\n",
      "Loss: 5.0986495\n",
      "Loss: 5.125308\n",
      "Loss: 5.170536\n",
      "Loss: 5.2131295\n",
      "Loss: 5.2045712\n",
      "Loss: 5.1483808\n",
      "Loss: 5.1015115\n",
      "Loss: 5.064302\n",
      "Loss: 5.045292\n",
      "Loss: 5.0309973\n",
      "Loss: 5.025241\n",
      "Loss: 5.0212955\n",
      "Loss: 5.025362\n",
      "Loss: 5.0372243\n",
      "Loss: 5.0659513\n",
      "Loss: 5.106113\n",
      "Loss: 5.12694\n",
      "Loss: 5.0986986\n",
      "Loss: 5.0567665\n",
      "Loss: 5.016356\n",
      "Loss: 4.9978957\n",
      "Loss: 4.993956\n",
      "Loss: 5.0031147\n",
      "Loss: 5.034551\n",
      "Loss: 5.0772347\n",
      "Loss: 5.097932\n",
      "Loss: 5.0670204\n",
      "Loss: 5.022701\n",
      "Loss: 4.9806004\n",
      "Loss: 4.954975\n",
      "Loss: 4.937782\n",
      "Loss: 4.9272604\n",
      "Loss: 4.920576\n",
      "Loss: 4.9202647\n",
      "Loss: 4.923661\n",
      "Loss: 4.93151\n",
      "Loss: 4.9431567\n",
      "Loss: 4.962638\n",
      "Loss: 4.9820733\n",
      "Loss: 5.003067\n",
      "Loss: 5.008317\n",
      "Loss: 4.98464\n",
      "Loss: 4.949395\n",
      "Loss: 4.912736\n",
      "Loss: 4.8876743\n",
      "Loss: 4.8731775\n",
      "Loss: 4.8733473\n",
      "Loss: 4.8790803\n",
      "Loss: 4.8971214\n",
      "Loss: 4.914468\n",
      "Loss: 4.924284\n",
      "Loss: 4.909356\n",
      "Loss: 4.887017\n",
      "Loss: 4.8640122\n",
      "Loss: 4.8475027\n",
      "Loss: 4.8384175\n",
      "Loss: 4.8468037\n",
      "Loss: 4.866256\n",
      "Loss: 4.886961\n",
      "Loss: 4.900511\n",
      "Loss: 4.8766203\n",
      "Loss: 4.8418503\n",
      "Loss: 4.811208\n",
      "Loss: 4.7903395\n",
      "Loss: 4.773265\n",
      "Loss: 4.765878\n",
      "Loss: 4.762414\n",
      "Loss: 4.76277\n",
      "Loss: 4.7688684\n",
      "Loss: 4.792947\n",
      "Loss: 4.8219695\n",
      "Loss: 4.8637605\n",
      "Loss: 4.8903737\n",
      "Loss: 4.856505\n",
      "Loss: 4.8076987\n",
      "Loss: 4.7574186\n",
      "Loss: 4.735063\n",
      "Loss: 4.7288036\n",
      "Loss: 4.7428718\n",
      "Loss: 4.7667537\n",
      "Loss: 4.779778\n",
      "Loss: 4.7614136\n",
      "Loss: 4.7362156\n",
      "Loss: 4.7092385\n",
      "Loss: 4.6916947\n",
      "Loss: 4.6830544\n",
      "Loss: 4.687158\n",
      "Loss: 4.695833\n",
      "Loss: 4.72929\n",
      "Loss: 4.774207\n",
      "Loss: 4.8002925\n",
      "Loss: 4.7795177\n",
      "Loss: 4.7227654\n",
      "Loss: 4.6809654\n",
      "Loss: 4.650498\n",
      "Loss: 4.6314015\n",
      "Loss: 4.6225886\n",
      "Loss: 4.6168084\n",
      "Loss: 4.6160474\n",
      "Loss: 4.626283\n",
      "Loss: 4.65085\n",
      "Loss: 4.6928153\n",
      "Loss: 4.7287874\n",
      "Loss: 4.7222815\n",
      "Loss: 4.673133\n",
      "Loss: 4.63319\n",
      "Loss: 4.6064663\n",
      "Loss: 4.5977154\n",
      "Loss: 4.604732\n",
      "Loss: 4.631555\n",
      "Loss: 4.6664586\n",
      "Loss: 4.6901684\n",
      "Loss: 4.680358\n",
      "Loss: 4.6357875\n",
      "Loss: 4.597949\n",
      "Loss: 4.5686975\n",
      "Loss: 4.5504866\n",
      "Loss: 4.536692\n",
      "Loss: 4.5300183\n",
      "Loss: 4.5251627\n",
      "Loss: 4.5278335\n",
      "Loss: 4.535618\n",
      "Loss: 4.5481405\n",
      "Loss: 4.5656924\n",
      "Loss: 4.5878944\n",
      "Loss: 4.614688\n",
      "Loss: 4.6327972\n",
      "Loss: 4.616416\n",
      "Loss: 4.5711746\n",
      "Loss: 4.531864\n",
      "Loss: 4.5068293\n",
      "Loss: 4.493116\n",
      "Loss: 4.489903\n",
      "Loss: 4.4992375\n",
      "Loss: 4.514405\n",
      "Loss: 4.5317464\n",
      "Loss: 4.5353966\n",
      "Loss: 4.522171\n",
      "Loss: 4.4986806\n",
      "Loss: 4.480427\n",
      "Loss: 4.4654775\n",
      "Loss: 4.4673257\n",
      "Loss: 4.474959\n",
      "Loss: 4.4953036\n",
      "Loss: 4.518729\n",
      "Loss: 4.525184\n",
      "Loss: 4.506582\n",
      "Loss: 4.47144\n",
      "Loss: 4.4392548\n",
      "Loss: 4.417554\n",
      "Loss: 4.401043\n",
      "Loss: 4.390216\n",
      "Loss: 4.383327\n",
      "Loss: 4.385293\n",
      "Loss: 4.388523\n",
      "Loss: 4.4011884\n",
      "Loss: 4.4119287\n",
      "Loss: 4.4215913\n",
      "Loss: 4.424148\n",
      "Loss: 4.418735\n",
      "Loss: 4.418016\n",
      "Loss: 4.43761\n",
      "Loss: 4.5118413\n",
      "Loss: 4.5743995\n",
      "Loss: 4.491644\n",
      "Loss: 4.4039364\n",
      "Loss: 4.348232\n",
      "Loss: 4.326098\n",
      "Loss: 4.3151393\n",
      "Loss: 4.3166475\n",
      "Loss: 4.3293996\n",
      "Loss: 4.3476934\n",
      "Loss: 4.365006\n",
      "Loss: 4.375289\n",
      "Loss: 4.3611817\n",
      "Loss: 4.3431735\n",
      "Loss: 4.3230643\n",
      "Loss: 4.3122644\n",
      "Loss: 4.3168454\n",
      "Loss: 4.3475432\n",
      "Loss: 4.398633\n",
      "Loss: 4.431695\n",
      "Loss: 4.4138465\n",
      "Loss: 4.3521066\n",
      "Loss: 4.3049817\n",
      "Loss: 4.26855\n",
      "Loss: 4.2515187\n",
      "Loss: 4.242988\n",
      "Loss: 4.2368145\n",
      "Loss: 4.235749\n",
      "Loss: 4.241808\n",
      "Loss: 4.2560167\n",
      "Loss: 4.2881994\n",
      "Loss: 4.3419456\n",
      "Loss: 4.378759\n",
      "Loss: 4.3434987\n",
      "Loss: 4.2958627\n",
      "Loss: 4.2525754\n",
      "Loss: 4.236772\n",
      "Loss: 4.236446\n",
      "Loss: 4.2598715\n",
      "Loss: 4.293722\n",
      "Loss: 4.3136373\n",
      "Loss: 4.304725\n",
      "Loss: 4.267523\n",
      "Loss: 4.233702\n",
      "Loss: 4.2024827\n",
      "Loss: 4.1855717\n",
      "Loss: 4.1724553\n",
      "Loss: 4.165854\n",
      "Loss: 4.1622066\n",
      "Loss: 4.1691513\n",
      "Loss: 4.1792717\n",
      "Loss: 4.1968336\n",
      "Loss: 4.220438\n",
      "Loss: 4.2519197\n",
      "Loss: 4.272837\n",
      "Loss: 4.260492\n",
      "Loss: 4.2296968\n",
      "Loss: 4.190965\n",
      "Loss: 4.161437\n",
      "Loss: 4.145397\n",
      "Loss: 4.1470475\n",
      "Loss: 4.159803\n",
      "Loss: 4.1790247\n",
      "Loss: 4.1904407\n",
      "Loss: 4.1916804\n",
      "Loss: 4.1700296\n",
      "Loss: 4.1477904\n",
      "Loss: 4.124404\n",
      "Loss: 4.115492\n",
      "Loss: 4.117091\n",
      "Loss: 4.1252127\n",
      "Loss: 4.1467714\n",
      "Loss: 4.1719303\n",
      "Loss: 4.1873283\n",
      "Loss: 4.1703215\n",
      "Loss: 4.138656\n",
      "Loss: 4.1047664\n",
      "Loss: 4.079644\n",
      "Loss: 4.062757\n",
      "Loss: 4.0530167\n",
      "Loss: 4.046691\n",
      "Loss: 4.047285\n",
      "Loss: 4.054156\n",
      "Loss: 4.06682\n",
      "Loss: 4.0821595\n",
      "Loss: 4.100284\n",
      "Loss: 4.11291\n",
      "Loss: 4.1378274\n",
      "Loss: 4.1648316\n",
      "Loss: 4.176949\n",
      "Loss: 4.1376014\n",
      "Loss: 4.098913\n",
      "Loss: 4.062787\n",
      "Loss: 4.04679\n",
      "Loss: 4.039288\n",
      "Loss: 4.0385966\n",
      "Loss: 4.03962\n",
      "Loss: 4.042054\n",
      "Loss: 4.0391607\n",
      "Loss: 4.031755\n",
      "Loss: 4.0242248\n",
      "Loss: 4.0134983\n",
      "Loss: 4.003567\n",
      "Loss: 3.9984348\n",
      "Loss: 3.9950695\n",
      "Loss: 3.9980054\n",
      "Loss: 4.005554\n",
      "Loss: 4.023037\n",
      "Loss: 4.0473533\n",
      "Loss: 4.0823274\n",
      "Loss: 4.074858\n",
      "Loss: 4.047329\n",
      "Loss: 4.0021544\n",
      "Loss: 3.9756095\n",
      "Loss: 3.9677958\n",
      "Loss: 3.9794018\n"
     ]
    }
   ],
   "source": [
    "for _ in 0..<1000 {\n",
    "    let 𝛁model = model.gradient { r -> Tensor<Float> in\n",
    "        let ŷ = r(XTrain)\n",
    "        let loss = meanSquaredError(predicted: ŷ, expected: YTrain)\n",
    "        print(\"Loss: \\(loss)\")\n",
    "        return loss\n",
    "    }\n",
    "    optimizer.update(&model, along: 𝛁model)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkf29HLlohIP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "S4TF_House.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

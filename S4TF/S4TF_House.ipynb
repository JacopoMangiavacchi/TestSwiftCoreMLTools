{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO4FT9bUohHx"
   },
   "outputs": [],
   "source": [
    "import Foundation\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "RgQazsBcohH5",
    "outputId": "c5f63b76-3009-4725-ef9d-caccf24671c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 14 101 14\r\n",
      "405 13 405 1 101 13 101 1\r\n",
      "5265 405 1313 101\r\n"
     ]
    }
   ],
   "source": [
    "let trainCSV = try String(contentsOfFile:\"../data/train.csv\", encoding: String.Encoding.utf8)\n",
    "let testCSV = try String(contentsOfFile:\"../data/test.csv\", encoding: String.Encoding.utf8)\n",
    "\n",
    "let trainRecords: [[Float]] = trainCSV.split(separator: \"\\n\").map{ String($0).split(separator: \",\").compactMap{ Float(String($0)) } }\n",
    "let testRecords: [[Float]] = testCSV.split(separator: \"\\n\").map{ String($0).split(separator: \",\").compactMap{ Float(String($0)) } }\n",
    "\n",
    "let numTrainRecords = trainRecords.count\n",
    "let numTrainColumns = trainRecords[0].count\n",
    "let numTestRecords = testRecords.count\n",
    "let numTestColumns = testRecords[0].count\n",
    "\n",
    "print(numTrainRecords, numTrainColumns, numTestRecords, numTestColumns)\n",
    "\n",
    "let xTrain = trainRecords.map{ Array($0[0..<numTrainColumns-1]) }\n",
    "let yTrain = trainRecords.map{ [$0[numTrainColumns-1]] }\n",
    "let xTest = testRecords.map{ Array($0[0..<numTestColumns-1]) }\n",
    "let yTest = testRecords.map{ [$0[numTestColumns-1]] }\n",
    "\n",
    "print(xTrain.count, xTrain[0].count, yTrain.count, yTrain[0].count,\n",
    "      xTest.count, xTest[0].count, yTest.count, yTest[0].count)\n",
    "\n",
    "let xAllTrain = Array(xTrain.joined())\n",
    "let yAllTrain = Array(yTrain.joined())\n",
    "let xAllTest = Array(xTest.joined())\n",
    "let yAllTest = Array(yTest.joined())\n",
    "\n",
    "print(xAllTrain.count, yAllTrain.count, xAllTest.count, yAllTest.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "qlhl7tOaohH_",
    "outputId": "1eef7f0c-fa9d-4f83-a1a0-8ea555fbe3b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[405, 13] [405, 1] [101, 13] [101, 1]\r\n"
     ]
    }
   ],
   "source": [
    "let XTrain = Tensor<Float>(xAllTrain).reshaped(to: TensorShape([numTrainRecords, numTrainColumns-1]))\n",
    "let YTrain = Tensor<Float>(yAllTrain).reshaped(to: TensorShape([numTrainRecords, 1]))\n",
    "let XTest = Tensor<Float>(xAllTest).reshaped(to: TensorShape([numTestRecords, numTestColumns-1]))\n",
    "let YTest = Tensor<Float>(yAllTest).reshaped(to: TensorShape([numTestRecords, 1]))\n",
    "\n",
    "print(XTrain.shape, YTrain.shape, XTest.shape, YTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDJZCCgqohIC"
   },
   "outputs": [],
   "source": [
    "struct RegressionModel: Layer {\n",
    "    var layer1 = Dense<Float>(inputSize: 13, outputSize: 64, activation: relu)\n",
    "    var layer2 = Dense<Float>(inputSize: 64, outputSize: 32, activation: relu)\n",
    "    var layer3 = Dense<Float>(inputSize: 32, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
    "        return input.sequenced(through: layer1, layer2, layer3)\n",
    "    }\n",
    "}\n",
    "\n",
    "var model = RegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JK0Vj7bSohIF"
   },
   "outputs": [],
   "source": [
    "let optimizer = RMSProp(for: model, learningRate: 0.001)\n",
    "Context.local.learningPhase = .training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8C7yHJohIH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 598.97925\n",
      "Loss: 589.64966\n",
      "Loss: 583.1557\n",
      "Loss: 577.6413\n",
      "Loss: 572.6768\n",
      "Loss: 568.08374\n",
      "Loss: 563.75214\n",
      "Loss: 559.5916\n",
      "Loss: 555.56256\n",
      "Loss: 551.614\n",
      "Loss: 547.73157\n",
      "Loss: 543.88666\n",
      "Loss: 540.05084\n",
      "Loss: 536.2087\n",
      "Loss: 532.3809\n",
      "Loss: 528.542\n",
      "Loss: 524.6906\n",
      "Loss: 520.7985\n",
      "Loss: 516.87396\n",
      "Loss: 512.92285\n",
      "Loss: 508.94128\n",
      "Loss: 504.91336\n",
      "Loss: 500.83905\n",
      "Loss: 496.71423\n",
      "Loss: 492.52985\n",
      "Loss: 488.2884\n",
      "Loss: 483.97836\n",
      "Loss: 479.61392\n",
      "Loss: 475.2012\n",
      "Loss: 470.74048\n",
      "Loss: 466.21082\n",
      "Loss: 461.62036\n",
      "Loss: 456.96802\n",
      "Loss: 452.26334\n",
      "Loss: 447.50873\n",
      "Loss: 442.69913\n",
      "Loss: 437.83444\n",
      "Loss: 432.91867\n",
      "Loss: 427.95035\n",
      "Loss: 422.92178\n",
      "Loss: 417.84467\n",
      "Loss: 412.71774\n",
      "Loss: 407.53882\n",
      "Loss: 402.3148\n",
      "Loss: 397.04532\n",
      "Loss: 391.72684\n",
      "Loss: 386.36102\n",
      "Loss: 380.95633\n",
      "Loss: 375.51498\n",
      "Loss: 370.03845\n",
      "Loss: 364.52872\n",
      "Loss: 358.994\n",
      "Loss: 353.43283\n",
      "Loss: 347.8434\n",
      "Loss: 342.23413\n",
      "Loss: 336.61008\n",
      "Loss: 330.9728\n",
      "Loss: 325.3237\n",
      "Loss: 319.6651\n",
      "Loss: 314.00082\n",
      "Loss: 308.33313\n",
      "Loss: 302.66708\n",
      "Loss: 297.00143\n",
      "Loss: 291.34537\n",
      "Loss: 285.70172\n",
      "Loss: 280.07373\n",
      "Loss: 274.46777\n",
      "Loss: 268.88678\n",
      "Loss: 263.3311\n",
      "Loss: 257.80588\n",
      "Loss: 252.31715\n",
      "Loss: 246.86687\n",
      "Loss: 241.45757\n",
      "Loss: 236.0931\n",
      "Loss: 230.78023\n",
      "Loss: 225.52069\n",
      "Loss: 220.31696\n",
      "Loss: 215.17041\n",
      "Loss: 210.08719\n",
      "Loss: 205.0726\n",
      "Loss: 200.12857\n",
      "Loss: 195.25687\n",
      "Loss: 190.46161\n",
      "Loss: 185.74811\n",
      "Loss: 181.11601\n",
      "Loss: 176.56467\n",
      "Loss: 172.0955\n",
      "Loss: 167.70757\n",
      "Loss: 163.4042\n",
      "Loss: 159.18161\n",
      "Loss: 155.04874\n",
      "Loss: 151.00427\n",
      "Loss: 147.05379\n",
      "Loss: 143.19194\n",
      "Loss: 139.42094\n",
      "Loss: 135.74225\n",
      "Loss: 132.15485\n",
      "Loss: 128.65857\n",
      "Loss: 125.253845\n",
      "Loss: 121.940025\n",
      "Loss: 118.71566\n",
      "Loss: 115.58232\n",
      "Loss: 112.537895\n",
      "Loss: 109.5814\n",
      "Loss: 106.70882\n",
      "Loss: 103.92236\n",
      "Loss: 101.222084\n",
      "Loss: 98.604324\n",
      "Loss: 96.06683\n",
      "Loss: 93.60762\n",
      "Loss: 91.22638\n",
      "Loss: 88.92603\n",
      "Loss: 86.70214\n",
      "Loss: 84.553894\n",
      "Loss: 82.47645\n",
      "Loss: 80.463844\n",
      "Loss: 78.52355\n",
      "Loss: 76.65532\n",
      "Loss: 74.85431\n",
      "Loss: 73.11595\n",
      "Loss: 71.43827\n",
      "Loss: 69.82016\n",
      "Loss: 68.25806\n",
      "Loss: 66.75019\n",
      "Loss: 65.29353\n",
      "Loss: 63.8859\n",
      "Loss: 62.524918\n",
      "Loss: 61.207268\n",
      "Loss: 59.93022\n",
      "Loss: 58.69306\n",
      "Loss: 57.492767\n",
      "Loss: 56.326992\n",
      "Loss: 55.194088\n",
      "Loss: 54.09386\n",
      "Loss: 53.021393\n",
      "Loss: 51.975414\n",
      "Loss: 50.955776\n",
      "Loss: 49.961197\n",
      "Loss: 48.992004\n",
      "Loss: 48.047867\n",
      "Loss: 47.127056\n",
      "Loss: 46.225018\n",
      "Loss: 45.34415\n",
      "Loss: 44.481693\n",
      "Loss: 43.640076\n",
      "Loss: 42.819965\n",
      "Loss: 42.021496\n",
      "Loss: 41.244156\n",
      "Loss: 40.486217\n",
      "Loss: 39.75028\n",
      "Loss: 39.03629\n",
      "Loss: 38.343018\n",
      "Loss: 37.670235\n",
      "Loss: 37.01689\n",
      "Loss: 36.383976\n",
      "Loss: 35.769836\n",
      "Loss: 35.17377\n",
      "Loss: 34.595207\n",
      "Loss: 34.03607\n",
      "Loss: 33.494747\n",
      "Loss: 32.968254\n",
      "Loss: 32.458008\n",
      "Loss: 31.963575\n",
      "Loss: 31.484386\n",
      "Loss: 31.019821\n",
      "Loss: 30.56937\n",
      "Loss: 30.133915\n",
      "Loss: 29.711794\n",
      "Loss: 29.302225\n",
      "Loss: 28.904945\n",
      "Loss: 28.520876\n",
      "Loss: 28.14862\n",
      "Loss: 27.788206\n",
      "Loss: 27.439022\n",
      "Loss: 27.098331\n",
      "Loss: 26.76778\n",
      "Loss: 26.447336\n",
      "Loss: 26.136402\n",
      "Loss: 25.834602\n",
      "Loss: 25.538414\n",
      "Loss: 25.249577\n",
      "Loss: 24.967867\n",
      "Loss: 24.691769\n",
      "Loss: 24.424139\n",
      "Loss: 24.161121\n",
      "Loss: 23.903967\n",
      "Loss: 23.65323\n",
      "Loss: 23.406755\n",
      "Loss: 23.165203\n",
      "Loss: 22.929625\n",
      "Loss: 22.697493\n",
      "Loss: 22.470057\n",
      "Loss: 22.246504\n",
      "Loss: 22.027645\n",
      "Loss: 21.812408\n",
      "Loss: 21.600616\n",
      "Loss: 21.390724\n",
      "Loss: 21.18405\n",
      "Loss: 20.982233\n",
      "Loss: 20.783722\n",
      "Loss: 20.587296\n",
      "Loss: 20.394438\n",
      "Loss: 20.20374\n",
      "Loss: 20.016096\n",
      "Loss: 19.830767\n",
      "Loss: 19.647985\n",
      "Loss: 19.467823\n",
      "Loss: 19.290373\n",
      "Loss: 19.122929\n",
      "Loss: 18.969942\n",
      "Loss: 18.828346\n",
      "Loss: 18.66499\n",
      "Loss: 18.493362\n",
      "Loss: 18.318295\n",
      "Loss: 18.154524\n",
      "Loss: 17.995577\n",
      "Loss: 17.841568\n",
      "Loss: 17.689964\n",
      "Loss: 17.542048\n",
      "Loss: 17.395563\n",
      "Loss: 17.251217\n",
      "Loss: 17.109238\n",
      "Loss: 16.965933\n",
      "Loss: 16.823917\n",
      "Loss: 16.68717\n",
      "Loss: 16.553709\n",
      "Loss: 16.426664\n",
      "Loss: 16.296347\n",
      "Loss: 16.163687\n",
      "Loss: 16.029257\n",
      "Loss: 15.902128\n",
      "Loss: 15.77457\n",
      "Loss: 15.655027\n",
      "Loss: 15.537913\n",
      "Loss: 15.424406\n",
      "Loss: 15.30799\n",
      "Loss: 15.201009\n",
      "Loss: 15.093892\n",
      "Loss: 14.990923\n",
      "Loss: 14.891077\n",
      "Loss: 14.797573\n",
      "Loss: 14.701354\n",
      "Loss: 14.610119\n",
      "Loss: 14.516931\n",
      "Loss: 14.428123\n",
      "Loss: 14.334542\n",
      "Loss: 14.246622\n",
      "Loss: 14.15579\n",
      "Loss: 14.070537\n",
      "Loss: 13.983675\n",
      "Loss: 13.901618\n",
      "Loss: 13.819445\n",
      "Loss: 13.7399025\n",
      "Loss: 13.659248\n",
      "Loss: 13.577985\n",
      "Loss: 13.496717\n",
      "Loss: 13.41864\n",
      "Loss: 13.340336\n",
      "Loss: 13.265693\n",
      "Loss: 13.188718\n",
      "Loss: 13.117291\n",
      "Loss: 13.04494\n",
      "Loss: 12.974933\n",
      "Loss: 12.900818\n",
      "Loss: 12.83224\n",
      "Loss: 12.760673\n",
      "Loss: 12.692979\n",
      "Loss: 12.625055\n",
      "Loss: 12.559982\n",
      "Loss: 12.493795\n",
      "Loss: 12.431854\n",
      "Loss: 12.3686695\n",
      "Loss: 12.307253\n",
      "Loss: 12.243789\n",
      "Loss: 12.184264\n",
      "Loss: 12.122438\n",
      "Loss: 12.062742\n",
      "Loss: 12.001196\n",
      "Loss: 11.943717\n",
      "Loss: 11.885053\n",
      "Loss: 11.828289\n",
      "Loss: 11.772205\n",
      "Loss: 11.716147\n",
      "Loss: 11.661677\n",
      "Loss: 11.610727\n",
      "Loss: 11.55752\n",
      "Loss: 11.507257\n",
      "Loss: 11.45854\n",
      "Loss: 11.410333\n",
      "Loss: 11.363115\n",
      "Loss: 11.312173\n",
      "Loss: 11.26133\n",
      "Loss: 11.210239\n",
      "Loss: 11.158784\n",
      "Loss: 11.114249\n",
      "Loss: 11.068604\n",
      "Loss: 11.0280695\n",
      "Loss: 10.9888735\n",
      "Loss: 10.951995\n",
      "Loss: 10.909005\n",
      "Loss: 10.870324\n",
      "Loss: 10.819273\n",
      "Loss: 10.774592\n",
      "Loss: 10.73052\n",
      "Loss: 10.690764\n",
      "Loss: 10.644112\n",
      "Loss: 10.605582\n",
      "Loss: 10.557134\n",
      "Loss: 10.51803\n",
      "Loss: 10.471731\n",
      "Loss: 10.42694\n",
      "Loss: 10.388869\n",
      "Loss: 10.356143\n",
      "Loss: 10.321934\n",
      "Loss: 10.295076\n",
      "Loss: 10.276517\n",
      "Loss: 10.270023\n",
      "Loss: 10.248682\n",
      "Loss: 10.216293\n",
      "Loss: 10.160859\n",
      "Loss: 10.117526\n",
      "Loss: 10.083166\n",
      "Loss: 10.047041\n",
      "Loss: 10.00792\n",
      "Loss: 9.969661\n",
      "Loss: 9.93038\n",
      "Loss: 9.896616\n",
      "Loss: 9.861613\n",
      "Loss: 9.830269\n",
      "Loss: 9.799012\n",
      "Loss: 9.773386\n",
      "Loss: 9.749909\n",
      "Loss: 9.733632\n",
      "Loss: 9.717494\n",
      "Loss: 9.70567\n",
      "Loss: 9.677743\n",
      "Loss: 9.642094\n",
      "Loss: 9.596063\n",
      "Loss: 9.560018\n",
      "Loss: 9.522444\n",
      "Loss: 9.4944725\n",
      "Loss: 9.474261\n",
      "Loss: 9.45988\n",
      "Loss: 9.446031\n",
      "Loss: 9.416229\n",
      "Loss: 9.384367\n",
      "Loss: 9.350231\n",
      "Loss: 9.320138\n",
      "Loss: 9.290144\n",
      "Loss: 9.26491\n",
      "Loss: 9.245273\n",
      "Loss: 9.2288\n",
      "Loss: 9.216251\n",
      "Loss: 9.215772\n",
      "Loss: 9.213732\n",
      "Loss: 9.190324\n",
      "Loss: 9.153841\n",
      "Loss: 9.115361\n",
      "Loss: 9.098022\n",
      "Loss: 9.078442\n",
      "Loss: 9.054854\n",
      "Loss: 9.016964\n",
      "Loss: 8.984995\n",
      "Loss: 8.954173\n",
      "Loss: 8.931802\n",
      "Loss: 8.909596\n",
      "Loss: 8.888573\n",
      "Loss: 8.864649\n",
      "Loss: 8.844377\n",
      "Loss: 8.8295145\n",
      "Loss: 8.826426\n",
      "Loss: 8.828468\n",
      "Loss: 8.831512\n",
      "Loss: 8.805528\n",
      "Loss: 8.763718\n",
      "Loss: 8.720098\n",
      "Loss: 8.686511\n",
      "Loss: 8.662119\n",
      "Loss: 8.644961\n",
      "Loss: 8.635776\n",
      "Loss: 8.626449\n",
      "Loss: 8.604\n",
      "Loss: 8.570212\n",
      "Loss: 8.541868\n",
      "Loss: 8.515772\n",
      "Loss: 8.496161\n",
      "Loss: 8.473622\n",
      "Loss: 8.454087\n",
      "Loss: 8.439404\n",
      "Loss: 8.430464\n",
      "Loss: 8.4292965\n",
      "Loss: 8.440899\n",
      "Loss: 8.468938\n",
      "Loss: 8.447413\n",
      "Loss: 8.396743\n",
      "Loss: 8.343625\n",
      "Loss: 8.313511\n",
      "Loss: 8.29907\n",
      "Loss: 8.293574\n",
      "Loss: 8.279352\n",
      "Loss: 8.261302\n",
      "Loss: 8.230894\n",
      "Loss: 8.204086\n",
      "Loss: 8.179448\n",
      "Loss: 8.161153\n",
      "Loss: 8.139779\n",
      "Loss: 8.124237\n",
      "Loss: 8.1112585\n",
      "Loss: 8.104236\n",
      "Loss: 8.108085\n",
      "Loss: 8.117928\n",
      "Loss: 8.118353\n",
      "Loss: 8.101372\n",
      "Loss: 8.056779\n",
      "Loss: 8.021046\n",
      "Loss: 7.9871583\n",
      "Loss: 7.968058\n",
      "Loss: 7.95681\n",
      "Loss: 7.9469376\n",
      "Loss: 7.945358\n",
      "Loss: 7.9324284\n",
      "Loss: 7.9107475\n",
      "Loss: 7.8806834\n",
      "Loss: 7.8515086\n",
      "Loss: 7.8274155\n",
      "Loss: 7.808836\n",
      "Loss: 7.7933545\n",
      "Loss: 7.7805176\n",
      "Loss: 7.768465\n",
      "Loss: 7.763784\n",
      "Loss: 7.7685843\n",
      "Loss: 7.7876754\n",
      "Loss: 7.7821994\n",
      "Loss: 7.7337203\n",
      "Loss: 7.690601\n",
      "Loss: 7.655332\n",
      "Loss: 7.6344776\n",
      "Loss: 7.630527\n",
      "Loss: 7.6261024\n",
      "Loss: 7.6111956\n",
      "Loss: 7.5872145\n",
      "Loss: 7.5555177\n",
      "Loss: 7.5342884\n",
      "Loss: 7.509334\n",
      "Loss: 7.4911456\n",
      "Loss: 7.475155\n",
      "Loss: 7.464383\n",
      "Loss: 7.4541354\n",
      "Loss: 7.4484835\n",
      "Loss: 7.45012\n",
      "Loss: 7.4664974\n",
      "Loss: 7.473499\n",
      "Loss: 7.4615755\n",
      "Loss: 7.4182296\n",
      "Loss: 7.3822947\n",
      "Loss: 7.35611\n",
      "Loss: 7.3381705\n",
      "Loss: 7.32917\n",
      "Loss: 7.325772\n",
      "Loss: 7.319396\n",
      "Loss: 7.2979956\n",
      "Loss: 7.272988\n",
      "Loss: 7.246849\n",
      "Loss: 7.225899\n",
      "Loss: 7.207485\n",
      "Loss: 7.1953607\n",
      "Loss: 7.184234\n",
      "Loss: 7.175487\n",
      "Loss: 7.1679688\n",
      "Loss: 7.1644244\n",
      "Loss: 7.1700687\n",
      "Loss: 7.1910563\n",
      "Loss: 7.210737\n",
      "Loss: 7.1990404\n",
      "Loss: 7.1576877\n",
      "Loss: 7.1063952\n",
      "Loss: 7.079295\n",
      "Loss: 7.0695486\n",
      "Loss: 7.0660505\n",
      "Loss: 7.0615983\n",
      "Loss: 7.0464263\n",
      "Loss: 7.0248747\n",
      "Loss: 7.001892\n",
      "Loss: 6.9834967\n",
      "Loss: 6.9679675\n",
      "Loss: 6.953179\n",
      "Loss: 6.9422593\n",
      "Loss: 6.932284\n",
      "Loss: 6.9236174\n",
      "Loss: 6.9161716\n",
      "Loss: 6.9123716\n",
      "Loss: 6.915456\n",
      "Loss: 6.925701\n",
      "Loss: 6.941718\n",
      "Loss: 6.9451942\n",
      "Loss: 6.911049\n",
      "Loss: 6.873731\n",
      "Loss: 6.8392916\n",
      "Loss: 6.8213434\n",
      "Loss: 6.814407\n",
      "Loss: 6.810455\n",
      "Loss: 6.796255\n",
      "Loss: 6.7845297\n",
      "Loss: 6.7722793\n",
      "Loss: 6.7513485\n",
      "Loss: 6.730608\n",
      "Loss: 6.718864\n",
      "Loss: 6.706438\n",
      "Loss: 6.6974363\n",
      "Loss: 6.6846547\n",
      "Loss: 6.682888\n",
      "Loss: 6.6798816\n",
      "Loss: 6.674619\n",
      "Loss: 6.666926\n",
      "Loss: 6.660165\n",
      "Loss: 6.6584773\n",
      "Loss: 6.693685\n",
      "Loss: 6.7314568\n",
      "Loss: 6.704106\n",
      "Loss: 6.628489\n",
      "Loss: 6.5904684\n",
      "Loss: 6.5696645\n",
      "Loss: 6.5605135\n",
      "Loss: 6.5607066\n",
      "Loss: 6.561668\n",
      "Loss: 6.5554924\n",
      "Loss: 6.5420637\n",
      "Loss: 6.5189466\n",
      "Loss: 6.5019617\n",
      "Loss: 6.488838\n",
      "Loss: 6.4773865\n",
      "Loss: 6.467258\n",
      "Loss: 6.4606752\n",
      "Loss: 6.454992\n",
      "Loss: 6.4457097\n",
      "Loss: 6.4387555\n",
      "Loss: 6.4505806\n",
      "Loss: 6.4741416\n",
      "Loss: 6.4979005\n",
      "Loss: 6.486353\n",
      "Loss: 6.4493256\n",
      "Loss: 6.404415\n",
      "Loss: 6.3807344\n",
      "Loss: 6.371828\n",
      "Loss: 6.370325\n",
      "Loss: 6.3743725\n",
      "Loss: 6.3675056\n",
      "Loss: 6.3543396\n",
      "Loss: 6.331474\n",
      "Loss: 6.3074145\n",
      "Loss: 6.2889514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.278779\n",
      "Loss: 6.270871\n",
      "Loss: 6.264503\n",
      "Loss: 6.2602334\n",
      "Loss: 6.262123\n",
      "Loss: 6.2715893\n",
      "Loss: 6.286413\n",
      "Loss: 6.2910457\n",
      "Loss: 6.306396\n",
      "Loss: 6.3151894\n",
      "Loss: 6.2988935\n",
      "Loss: 6.2495594\n",
      "Loss: 6.217643\n",
      "Loss: 6.192269\n",
      "Loss: 6.175359\n",
      "Loss: 6.1671133\n",
      "Loss: 6.168162\n",
      "Loss: 6.1723094\n",
      "Loss: 6.177707\n",
      "Loss: 6.1838694\n",
      "Loss: 6.1637588\n",
      "Loss: 6.141991\n",
      "Loss: 6.1216555\n",
      "Loss: 6.1092286\n",
      "Loss: 6.0950527\n",
      "Loss: 6.086775\n",
      "Loss: 6.078113\n",
      "Loss: 6.0761757\n",
      "Loss: 6.0756145\n",
      "Loss: 6.077569\n",
      "Loss: 6.076438\n",
      "Loss: 6.0805545\n",
      "Loss: 6.0775948\n",
      "Loss: 6.0964565\n",
      "Loss: 6.131393\n",
      "Loss: 6.111755\n",
      "Loss: 6.0675564\n",
      "Loss: 6.022523\n",
      "Loss: 6.0064716\n",
      "Loss: 6.0010386\n",
      "Loss: 6.007497\n",
      "Loss: 6.003655\n",
      "Loss: 5.993161\n",
      "Loss: 5.975675\n",
      "Loss: 5.9640946\n",
      "Loss: 5.94493\n",
      "Loss: 5.931681\n",
      "Loss: 5.918462\n",
      "Loss: 5.914497\n",
      "Loss: 5.9089327\n",
      "Loss: 5.9121757\n",
      "Loss: 5.9111032\n",
      "Loss: 5.9195495\n",
      "Loss: 5.9178085\n",
      "Loss: 5.9227347\n",
      "Loss: 5.9264355\n",
      "Loss: 5.947149\n",
      "Loss: 5.951105\n",
      "Loss: 5.91375\n",
      "Loss: 5.874357\n",
      "Loss: 5.8396506\n",
      "Loss: 5.8234386\n",
      "Loss: 5.8137255\n",
      "Loss: 5.815068\n",
      "Loss: 5.823507\n",
      "Loss: 5.8504586\n",
      "Loss: 5.8432794\n",
      "Loss: 5.8311987\n",
      "Loss: 5.7980704\n",
      "Loss: 5.7778406\n",
      "Loss: 5.755181\n",
      "Loss: 5.7449055\n",
      "Loss: 5.7325954\n",
      "Loss: 5.7307315\n",
      "Loss: 5.724933\n",
      "Loss: 5.7290225\n",
      "Loss: 5.7291245\n",
      "Loss: 5.7342796\n",
      "Loss: 5.7289143\n",
      "Loss: 5.730261\n",
      "Loss: 5.712151\n",
      "Loss: 5.7019553\n",
      "Loss: 5.685835\n",
      "Loss: 5.6895156\n",
      "Loss: 5.7113504\n",
      "Loss: 5.7916336\n",
      "Loss: 5.7780676\n",
      "Loss: 5.7102857\n",
      "Loss: 5.6504345\n",
      "Loss: 5.625328\n",
      "Loss: 5.616391\n",
      "Loss: 5.6104755\n",
      "Loss: 5.617917\n",
      "Loss: 5.6250596\n",
      "Loss: 5.634724\n",
      "Loss: 5.621359\n",
      "Loss: 5.619389\n",
      "Loss: 5.592961\n",
      "Loss: 5.576914\n",
      "Loss: 5.5564947\n",
      "Loss: 5.5539694\n",
      "Loss: 5.54718\n",
      "Loss: 5.5488615\n",
      "Loss: 5.5482774\n",
      "Loss: 5.573783\n",
      "Loss: 5.595222\n",
      "Loss: 5.6170826\n",
      "Loss: 5.594899\n",
      "Loss: 5.5610237\n",
      "Loss: 5.5237894\n",
      "Loss: 5.5020475\n",
      "Loss: 5.489778\n",
      "Loss: 5.4848814\n",
      "Loss: 5.4900703\n",
      "Loss: 5.5007925\n",
      "Loss: 5.5215507\n",
      "Loss: 5.5100803\n",
      "Loss: 5.4968443\n",
      "Loss: 5.4633875\n",
      "Loss: 5.441419\n",
      "Loss: 5.4247475\n",
      "Loss: 5.4230576\n",
      "Loss: 5.4130173\n",
      "Loss: 5.413073\n",
      "Loss: 5.417458\n",
      "Loss: 5.440802\n",
      "Loss: 5.461149\n",
      "Loss: 5.48206\n",
      "Loss: 5.459714\n",
      "Loss: 5.428971\n",
      "Loss: 5.38983\n",
      "Loss: 5.367482\n",
      "Loss: 5.3498373\n",
      "Loss: 5.3448443\n",
      "Loss: 5.350803\n",
      "Loss: 5.360171\n",
      "Loss: 5.383613\n",
      "Loss: 5.3967896\n",
      "Loss: 5.395852\n",
      "Loss: 5.361286\n",
      "Loss: 5.33931\n",
      "Loss: 5.308432\n",
      "Loss: 5.2986813\n",
      "Loss: 5.2861447\n",
      "Loss: 5.2798886\n",
      "Loss: 5.2823176\n",
      "Loss: 5.29847\n",
      "Loss: 5.319596\n",
      "Loss: 5.329909\n",
      "Loss: 5.314196\n",
      "Loss: 5.2910676\n",
      "Loss: 5.25795\n",
      "Loss: 5.238558\n",
      "Loss: 5.221443\n",
      "Loss: 5.210907\n",
      "Loss: 5.2086954\n",
      "Loss: 5.2119346\n",
      "Loss: 5.2264743\n",
      "Loss: 5.2534046\n",
      "Loss: 5.293407\n",
      "Loss: 5.2721963\n",
      "Loss: 5.240973\n",
      "Loss: 5.194124\n",
      "Loss: 5.1750937\n",
      "Loss: 5.1583395\n",
      "Loss: 5.1584764\n",
      "Loss: 5.1562896\n",
      "Loss: 5.165879\n",
      "Loss: 5.18218\n",
      "Loss: 5.1949406\n",
      "Loss: 5.1843877\n",
      "Loss: 5.1705494\n",
      "Loss: 5.141154\n",
      "Loss: 5.1246347\n",
      "Loss: 5.1078534\n",
      "Loss: 5.097178\n",
      "Loss: 5.088418\n",
      "Loss: 5.08805\n",
      "Loss: 5.09314\n",
      "Loss: 5.104876\n",
      "Loss: 5.130832\n",
      "Loss: 5.157299\n",
      "Loss: 5.1704836\n",
      "Loss: 5.129853\n",
      "Loss: 5.0969768\n",
      "Loss: 5.059386\n",
      "Loss: 5.045416\n",
      "Loss: 5.0314674\n",
      "Loss: 5.0334\n",
      "Loss: 5.0360637\n",
      "Loss: 5.0549545\n",
      "Loss: 5.0567675\n",
      "Loss: 5.0603766\n",
      "Loss: 5.0446415\n",
      "Loss: 5.0305433\n",
      "Loss: 5.009985\n",
      "Loss: 4.9924273\n",
      "Loss: 4.980413\n",
      "Loss: 4.9759445\n",
      "Loss: 4.967115\n",
      "Loss: 4.9685726\n",
      "Loss: 4.970906\n",
      "Loss: 4.9941964\n",
      "Loss: 5.0163703\n",
      "Loss: 5.0413175\n",
      "Loss: 5.026878\n",
      "Loss: 5.012024\n",
      "Loss: 4.970399\n",
      "Loss: 4.958062\n",
      "Loss: 4.942452\n",
      "Loss: 4.933111\n",
      "Loss: 4.926097\n",
      "Loss: 4.919476\n",
      "Loss: 4.911206\n",
      "Loss: 4.901847\n",
      "Loss: 4.8912354\n",
      "Loss: 4.8817782\n",
      "Loss: 4.8767347\n",
      "Loss: 4.877522\n",
      "Loss: 4.878264\n",
      "Loss: 4.890347\n",
      "Loss: 4.9305277\n",
      "Loss: 4.976738\n",
      "Loss: 4.975438\n",
      "Loss: 4.9204044\n",
      "Loss: 4.878085\n",
      "Loss: 4.8443937\n",
      "Loss: 4.832648\n",
      "Loss: 4.822788\n",
      "Loss: 4.828371\n",
      "Loss: 4.8332143\n",
      "Loss: 4.845723\n",
      "Loss: 4.847067\n",
      "Loss: 4.8469114\n",
      "Loss: 4.828454\n",
      "Loss: 4.8128657\n",
      "Loss: 4.7973638\n",
      "Loss: 4.786383\n",
      "Loss: 4.77552\n",
      "Loss: 4.767749\n",
      "Loss: 4.765435\n",
      "Loss: 4.765814\n",
      "Loss: 4.769305\n",
      "Loss: 4.77703\n",
      "Loss: 4.8107595\n",
      "Loss: 4.8819695\n",
      "Loss: 4.9153247\n",
      "Loss: 4.8485556\n",
      "Loss: 4.7781205\n",
      "Loss: 4.7346663\n",
      "Loss: 4.7192044\n",
      "Loss: 4.7069583\n",
      "Loss: 4.7035346\n",
      "Loss: 4.704839\n",
      "Loss: 4.715666\n",
      "Loss: 4.726894\n",
      "Loss: 4.7384815\n",
      "Loss: 4.736326\n",
      "Loss: 4.733547\n",
      "Loss: 4.7134037\n",
      "Loss: 4.6934943\n",
      "Loss: 4.6754227\n",
      "Loss: 4.665206\n",
      "Loss: 4.6562777\n",
      "Loss: 4.6545606\n",
      "Loss: 4.6567044\n",
      "Loss: 4.6613197\n",
      "Loss: 4.6619215\n",
      "Loss: 4.658533\n",
      "Loss: 4.6581783\n",
      "Loss: 4.6676784\n",
      "Loss: 4.692768\n",
      "Loss: 4.74976\n",
      "Loss: 4.7714515\n",
      "Loss: 4.741652\n",
      "Loss: 4.679005\n",
      "Loss: 4.641592\n",
      "Loss: 4.6113253\n",
      "Loss: 4.596678\n",
      "Loss: 4.5835605\n",
      "Loss: 4.5792074\n",
      "Loss: 4.574173\n",
      "Loss: 4.5795236\n",
      "Loss: 4.5780907\n",
      "Loss: 4.5842714\n",
      "Loss: 4.587815\n",
      "Loss: 4.597963\n",
      "Loss: 4.600946\n",
      "Loss: 4.606671\n",
      "Loss: 4.6033072\n",
      "Loss: 4.6063585\n",
      "Loss: 4.5932975\n",
      "Loss: 4.5947657\n",
      "Loss: 4.577464\n",
      "Loss: 4.576705\n",
      "Loss: 4.5595217\n",
      "Loss: 4.548931\n",
      "Loss: 4.535318\n",
      "Loss: 4.5258827\n",
      "Loss: 4.512455\n",
      "Loss: 4.505109\n",
      "Loss: 4.4976563\n",
      "Loss: 4.4914756\n",
      "Loss: 4.488835\n",
      "Loss: 4.4844804\n",
      "Loss: 4.485224\n",
      "Loss: 4.4816327\n",
      "Loss: 4.489367\n",
      "Loss: 4.5102262\n",
      "Loss: 4.5808268\n",
      "Loss: 4.6386538\n",
      "Loss: 4.6026917\n",
      "Loss: 4.520921\n",
      "Loss: 4.4707947\n",
      "Loss: 4.440969\n",
      "Loss: 4.4327874\n",
      "Loss: 4.4265513\n",
      "Loss: 4.4301515\n",
      "Loss: 4.4341254\n",
      "Loss: 4.4457455\n",
      "Loss: 4.454775\n",
      "Loss: 4.461715\n",
      "Loss: 4.4514155\n",
      "Loss: 4.435277\n",
      "Loss: 4.4173145\n",
      "Loss: 4.409288\n",
      "Loss: 4.3972025\n",
      "Loss: 4.390648\n",
      "Loss: 4.389985\n",
      "Loss: 4.3973517\n",
      "Loss: 4.4028225\n",
      "Loss: 4.4279366\n",
      "Loss: 4.455786\n",
      "Loss: 4.492615\n",
      "Loss: 4.489908\n",
      "Loss: 4.4594235\n",
      "Loss: 4.410968\n",
      "Loss: 4.3794255\n",
      "Loss: 4.35293\n",
      "Loss: 4.337927\n",
      "Loss: 4.3252087\n",
      "Loss: 4.3206177\n",
      "Loss: 4.316297\n",
      "Loss: 4.3177085\n",
      "Loss: 4.316912\n",
      "Loss: 4.325399\n",
      "Loss: 4.3337264\n",
      "Loss: 4.354252\n",
      "Loss: 4.3746\n",
      "Loss: 4.3998733\n",
      "Loss: 4.39871\n",
      "Loss: 4.381849\n",
      "Loss: 4.3521514\n",
      "Loss: 4.337207\n",
      "Loss: 4.3184714\n",
      "Loss: 4.310813\n",
      "Loss: 4.2983737\n",
      "Loss: 4.2866673\n",
      "Loss: 4.280428\n",
      "Loss: 4.271468\n",
      "Loss: 4.265392\n",
      "Loss: 4.2616363\n",
      "Loss: 4.2573204\n",
      "Loss: 4.2532907\n",
      "Loss: 4.2530355\n",
      "Loss: 4.2545033\n",
      "Loss: 4.261188\n",
      "Loss: 4.28365\n",
      "Loss: 4.3158326\n",
      "Loss: 4.346217\n",
      "Loss: 4.3498907\n",
      "Loss: 4.323271\n",
      "Loss: 4.2854314\n",
      "Loss: 4.25713\n",
      "Loss: 4.233465\n",
      "Loss: 4.220382\n",
      "Loss: 4.2069144\n",
      "Loss: 4.199413\n",
      "Loss: 4.191155\n",
      "Loss: 4.1869226\n",
      "Loss: 4.1832304\n",
      "Loss: 4.185835\n",
      "Loss: 4.193741\n",
      "Loss: 4.2066555\n",
      "Loss: 4.2267013\n",
      "Loss: 4.2484627\n",
      "Loss: 4.256056\n",
      "Loss: 4.253368\n",
      "Loss: 4.2265677\n",
      "Loss: 4.2153134\n",
      "Loss: 4.2000427\n",
      "Loss: 4.1907187\n",
      "Loss: 4.181033\n",
      "Loss: 4.1693134\n",
      "Loss: 4.157459\n",
      "Loss: 4.1440125\n",
      "Loss: 4.1359982\n",
      "Loss: 4.130884\n",
      "Loss: 4.123565\n",
      "Loss: 4.117334\n",
      "Loss: 4.1142564\n",
      "Loss: 4.1096163\n",
      "Loss: 4.1124306\n",
      "Loss: 4.112029\n",
      "Loss: 4.111183\n",
      "Loss: 4.1099186\n",
      "Loss: 4.1268444\n",
      "Loss: 4.1784687\n",
      "Loss: 4.2731314\n",
      "Loss: 4.2907524\n",
      "Loss: 4.2117643\n",
      "Loss: 4.13827\n",
      "Loss: 4.099162\n",
      "Loss: 4.077084\n",
      "Loss: 4.0642686\n",
      "Loss: 4.0592217\n",
      "Loss: 4.0566654\n",
      "Loss: 4.058311\n",
      "Loss: 4.0587726\n",
      "Loss: 4.0603204\n",
      "Loss: 4.064406\n",
      "Loss: 4.0692725\n",
      "Loss: 4.073781\n",
      "Loss: 4.080042\n",
      "Loss: 4.0875387\n",
      "Loss: 4.098861\n",
      "Loss: 4.1058264\n",
      "Loss: 4.1061735\n",
      "Loss: 4.091437\n",
      "Loss: 4.074816\n",
      "Loss: 4.056206\n",
      "Loss: 4.0404677\n",
      "Loss: 4.030384\n",
      "Loss: 4.0186634\n",
      "Loss: 4.0108814\n",
      "Loss: 4.005308\n",
      "Loss: 4.002608\n",
      "Loss: 3.9953246\n",
      "Loss: 3.990652\n",
      "Loss: 3.9894533\n",
      "Loss: 3.9873147\n",
      "Loss: 3.9800932\n",
      "Loss: 3.9818678\n",
      "Loss: 3.9848585\n",
      "Loss: 4.002445\n",
      "Loss: 4.0496354\n",
      "Loss: 4.119059\n",
      "Loss: 4.141776\n",
      "Loss: 4.085227\n"
     ]
    }
   ],
   "source": [
    "for _ in 0..<1000 {\n",
    "    let ùõÅmodel = model.gradient { r -> Tensor<Float> in\n",
    "        let ≈∑ = r(XTrain)\n",
    "        let loss = meanSquaredError(predicted: ≈∑, expected: YTrain)\n",
    "        print(\"Loss: \\(loss)\")\n",
    "        return loss\n",
    "    }\n",
    "    optimizer.update(&model, along: ùõÅmodel)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkf29HLlohIP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "S4TF_House.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
